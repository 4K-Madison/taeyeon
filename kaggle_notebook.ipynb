{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain-to-Text Model Evaluation Notebook\n",
    "\n",
    "This notebook is designed for Kaggle competitions and allows you to:\n",
    "1. Load pretrained RNN models\n",
    "2. Evaluate on test/validation data\n",
    "3. Generate predictions for submission\n",
    "\n",
    "Based on the NEJM Brain-to-Text paper (Card et al., 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q omegaconf pandas numpy h5py tqdm editdistance scipy g2p_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Project Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs('model_training', exist_ok=True)\n",
    "os.makedirs('nejm_b2txt_utils', exist_ok=True)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Add to path\n",
    "sys.path.append('/kaggle/working')\n",
    "sys.path.append('/kaggle/working/model_training')\n",
    "sys.path.append('/kaggle/working/nejm_b2txt_utils')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Copy Project Files\n",
    "\n",
    "**Note:** In Kaggle, you'll need to add the project files as datasets or copy them manually.\n",
    "This cell assumes the files are available in `/kaggle/input/` or you can upload them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy files from input dataset (adjust path as needed)\n",
    "# Uncomment and modify if you have the files in a Kaggle dataset\n",
    "\n",
    "# import shutil\n",
    "# \n",
    "# # Example: Copy from input dataset\n",
    "# # shutil.copytree('/kaggle/input/your-dataset-name/model_training', '/kaggle/working/model_training', dirs_exist_ok=True)\n",
    "# # shutil.copytree('/kaggle/input/your-dataset-name/nejm_b2txt_utils', '/kaggle/working/nejm_b2txt_utils', dirs_exist_ok=True)\n",
    "\n",
    "print(\"Files should be copied manually or via Kaggle dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import or Define Core Functions\n",
    "\n",
    "Try to import from project files first. If not available, essential functions are defined inline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import from files, otherwise define inline\n",
    "try:\n",
    "    from model_training.rnn_model import GRUDecoder\n",
    "    from model_training.evaluate_model_helpers import (\n",
    "        load_h5py_file, runSingleDecodingStep, \n",
    "        LOGIT_TO_PHONEME, rearrange_speech_logits_pt, remove_punctuation\n",
    "    )\n",
    "    from model_training.data_augmentations import gauss_smooth\n",
    "    print(\"✓ Successfully imported from project files\")\n",
    "except ImportError as e:\n",
    "    print(f\"Files not found ({e}), defining functions inline...\")\n",
    "    \n",
    "    # Import required libraries\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import numpy as np\n",
    "    import h5py\n",
    "    import re\n",
    "    from scipy.ndimage import gaussian_filter1d\n",
    "    \n",
    "    # Define phoneme mapping\n",
    "    LOGIT_TO_PHONEME = [\n",
    "        'BLANK',\n",
    "        'AA', 'AE', 'AH', 'AO', 'AW',\n",
    "        'AY', 'B',  'CH', 'D', 'DH',\n",
    "        'EH', 'ER', 'EY', 'F', 'G',\n",
    "        'HH', 'IH', 'IY', 'JH', 'K',\n",
    "        'L', 'M', 'N', 'NG', 'OW',\n",
    "        'OY', 'P', 'R', 'S', 'SH',\n",
    "        'T', 'TH', 'UH', 'UW', 'V',\n",
    "        'W', 'Y', 'Z', 'ZH',\n",
    "        ' | ',\n",
    "    ]\n",
    "    \n",
    "    # Define GRUDecoder (simplified version - see rnn_model.py for full implementation)\n",
    "    class GRUDecoder(nn.Module):\n",
    "        def __init__(self, neural_dim, n_units, n_days, n_classes, \n",
    "                     rnn_dropout=0.0, input_dropout=0.0, n_layers=5, \n",
    "                     patch_size=0, patch_stride=0):\n",
    "            super(GRUDecoder, self).__init__()\n",
    "            self.neural_dim = neural_dim\n",
    "            self.n_units = n_units\n",
    "            self.n_classes = n_classes\n",
    "            self.n_layers = n_layers\n",
    "            self.n_days = n_days\n",
    "            self.rnn_dropout = rnn_dropout\n",
    "            self.input_dropout = input_dropout\n",
    "            self.patch_size = patch_size\n",
    "            self.patch_stride = patch_stride\n",
    "            \n",
    "            self.day_layer_activation = nn.Softsign()\n",
    "            self.day_weights = nn.ParameterList(\n",
    "                [nn.Parameter(torch.eye(self.neural_dim)) for _ in range(self.n_days)]\n",
    "            )\n",
    "            self.day_biases = nn.ParameterList(\n",
    "                [nn.Parameter(torch.zeros(1, self.neural_dim)) for _ in range(self.n_days)]\n",
    "            )\n",
    "            self.day_layer_dropout = nn.Dropout(input_dropout)\n",
    "            self.input_size = self.neural_dim\n",
    "            \n",
    "            if self.patch_size > 0:\n",
    "                self.input_size *= self.patch_size\n",
    "            \n",
    "            self.gru = nn.GRU(\n",
    "                input_size=self.input_size,\n",
    "                hidden_size=self.n_units,\n",
    "                num_layers=self.n_layers,\n",
    "                dropout=self.rnn_dropout,\n",
    "                batch_first=True,\n",
    "                bidirectional=False,\n",
    "            )\n",
    "            \n",
    "            for name, param in self.gru.named_parameters():\n",
    "                if \"weight_hh\" in name:\n",
    "                    nn.init.orthogonal_(param)\n",
    "                if \"weight_ih\" in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "            \n",
    "            self.out = nn.Linear(self.n_units, self.n_classes)\n",
    "            nn.init.xavier_uniform_(self.out.weight)\n",
    "            self.h0 = nn.Parameter(nn.init.xavier_uniform_(torch.zeros(1, 1, self.n_units)))\n",
    "        \n",
    "        def forward(self, x, day_idx, states=None, return_state=False):\n",
    "            day_weights = torch.stack([self.day_weights[i] for i in day_idx], dim=0)\n",
    "            day_biases = torch.cat([self.day_biases[i] for i in day_idx], dim=0).unsqueeze(1)\n",
    "            x = torch.einsum(\"btd,bdk->btk\", x, day_weights) + day_biases\n",
    "            x = self.day_layer_activation(x)\n",
    "            if self.input_dropout > 0:\n",
    "                x = self.day_layer_dropout(x)\n",
    "            if self.patch_size > 0:\n",
    "                x = x.unsqueeze(1)\n",
    "                x = x.permute(0, 3, 1, 2)\n",
    "                x_unfold = x.unfold(3, self.patch_size, self.patch_stride)\n",
    "                x_unfold = x_unfold.squeeze(2)\n",
    "                x_unfold = x_unfold.permute(0, 2, 3, 1)\n",
    "                x = x_unfold.reshape(x.size(0), x_unfold.size(1), -1)\n",
    "            if states is None:\n",
    "                states = self.h0.expand(self.n_layers, x.shape[0], self.n_units).contiguous()\n",
    "            output, hidden_states = self.gru(x, states)\n",
    "            logits = self.out(output)\n",
    "            if return_state:\n",
    "                return logits, hidden_states\n",
    "            return logits\n",
    "    \n",
    "    # Helper functions\n",
    "    def gauss_smooth(inputs, device, smooth_kernel_std=2, smooth_kernel_size=100, padding='same'):\n",
    "        inp = np.zeros(smooth_kernel_size, dtype=np.float32)\n",
    "        inp[smooth_kernel_size // 2] = 1\n",
    "        gaussKernel = gaussian_filter1d(inp, smooth_kernel_std)\n",
    "        validIdx = np.argwhere(gaussKernel > 0.01)\n",
    "        gaussKernel = gaussKernel[validIdx]\n",
    "        gaussKernel = np.squeeze(gaussKernel / np.sum(gaussKernel))\n",
    "        gaussKernel = torch.tensor(gaussKernel, dtype=torch.float32, device=device)\n",
    "        gaussKernel = gaussKernel.view(1, 1, -1)\n",
    "        B, T, C = inputs.shape\n",
    "        inputs = inputs.permute(0, 2, 1)\n",
    "        gaussKernel = gaussKernel.repeat(C, 1, 1)\n",
    "        smoothed = F.conv1d(inputs, gaussKernel, padding=padding, groups=C)\n",
    "        return smoothed.permute(0, 2, 1)\n",
    "    \n",
    "    def load_h5py_file(file_path, b2txt_csv_df):\n",
    "        data = {\n",
    "            'neural_features': [], 'n_time_steps': [], 'seq_class_ids': [],\n",
    "            'seq_len': [], 'transcriptions': [], 'sentence_label': [],\n",
    "            'session': [], 'block_num': [], 'trial_num': [], 'corpus': [],\n",
    "        }\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            for key in list(f.keys()):\n",
    "                g = f[key]\n",
    "                year, month, day = g.attrs['session'].split('.')[1:]\n",
    "                date = f'{year}-{month}-{day}'\n",
    "                row = b2txt_csv_df[(b2txt_csv_df['Date'] == date) & \n",
    "                                   (b2txt_csv_df['Block number'] == g.attrs['block_num'])]\n",
    "                corpus_name = row['Corpus'].values[0] if len(row) > 0 else 'unknown'\n",
    "                data['neural_features'].append(g['input_features'][:])\n",
    "                data['n_time_steps'].append(g.attrs['n_time_steps'])\n",
    "                data['seq_class_ids'].append(g['seq_class_ids'][:] if 'seq_class_ids' in g else None)\n",
    "                data['seq_len'].append(g.attrs['seq_len'] if 'seq_len' in g.attrs else None)\n",
    "                data['transcriptions'].append(g['transcription'][:] if 'transcription' in g else None)\n",
    "                data['sentence_label'].append(g.attrs['sentence_label'][:] if 'sentence_label' in g.attrs else None)\n",
    "                data['session'].append(g.attrs['session'])\n",
    "                data['block_num'].append(g.attrs['block_num'])\n",
    "                data['trial_num'].append(g.attrs['trial_num'])\n",
    "                data['corpus'].append(corpus_name)\n",
    "        return data\n",
    "    \n",
    "    def rearrange_speech_logits_pt(logits):\n",
    "        return np.concatenate((logits[:, :, 0:1], logits[:, :, -1:], logits[:, :, 1:-1]), axis=-1)\n",
    "    \n",
    "    def remove_punctuation(sentence):\n",
    "        sentence = re.sub(r'[^a-zA-Z\\- \\']', '', sentence)\n",
    "        sentence = sentence.replace('- ', ' ').replace('--', '').replace(\" '\", \"'\").lower()\n",
    "        return ' '.join([word for word in sentence.strip().split() if word != ''])\n",
    "    \n",
    "    def runSingleDecodingStep(x, input_layer, model, model_args, device):\n",
    "        with torch.autocast(device_type=\"cuda\", enabled=model_args.get('use_amp', False), dtype=torch.bfloat16):\n",
    "            x = gauss_smooth(x, device,\n",
    "                smooth_kernel_std=model_args['dataset']['data_transforms']['smooth_kernel_std'],\n",
    "                smooth_kernel_size=model_args['dataset']['data_transforms']['smooth_kernel_size'],\n",
    "                padding='valid')\n",
    "            with torch.no_grad():\n",
    "                logits, _ = model(x=x, day_idx=torch.tensor([input_layer], device=device),\n",
    "                                states=None, return_state=True)\n",
    "        return logits.float().cpu().numpy()\n",
    "    \n",
    "    print(\"✓ Functions defined inline\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration - UPDATE THESE PATHS FOR YOUR KAGGLE SETUP\n",
    "CONFIG = {\n",
    "    'model_path': '/kaggle/input/your-model-path/t15_pretrained_rnn_baseline',  # Update this\n",
    "    'data_dir': '/kaggle/input/your-data-path/hdf5_data_final',  # Update this\n",
    "    'csv_path': '/kaggle/input/your-data-path/t15_copyTaskData_description.csv',  # Update this\n",
    "    'eval_type': 'test',  # 'val' or 'test'\n",
    "    'gpu_number': 0,  # GPU to use, or -1 for CPU\n",
    "    'output_file': 'predictions.csv'\n",
    "}\n",
    "\n",
    "# Set device\n",
    "if torch.cuda.is_available() and CONFIG['gpu_number'] >= 0:\n",
    "    device = torch.device(f'cuda:{CONFIG[\"gpu_number\"]}')\n",
    "    print(f'✓ Using {device} for model inference.')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('⚠ Using CPU for model inference.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model configuration\n",
    "model_args = OmegaConf.load(os.path.join(CONFIG['model_path'], 'checkpoint/args.yaml'))\n",
    "\n",
    "# Create model\n",
    "model = GRUDecoder(\n",
    "    neural_dim=model_args['model']['n_input_features'],\n",
    "    n_units=model_args['model']['n_units'],\n",
    "    n_days=len(model_args['dataset']['sessions']),\n",
    "    n_classes=model_args['dataset']['n_classes'],\n",
    "    rnn_dropout=model_args['model']['rnn_dropout'],\n",
    "    input_dropout=model_args['model']['input_network']['input_layer_dropout'],\n",
    "    n_layers=model_args['model']['n_layers'],\n",
    "    patch_size=model_args['model']['patch_size'],\n",
    "    patch_stride=model_args['model']['patch_stride'],\n",
    ")\n",
    "\n",
    "# Load model weights\n",
    "checkpoint = torch.load(\n",
    "    os.path.join(CONFIG['model_path'], 'checkpoint/best_checkpoint'),\n",
    "    weights_only=False,\n",
    "    map_location=device\n",
    ")\n",
    "\n",
    "# Handle DataParallel keys\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "new_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    new_key = key.replace('module.', '').replace('_orig_mod.', '')\n",
    "    new_state_dict[new_key] = value\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"✓ Model loaded successfully. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV metadata\n",
    "b2txt_csv_df = pd.read_csv(CONFIG['csv_path'])\n",
    "\n",
    "# Load data for each session\n",
    "test_data = {}\n",
    "total_test_trials = 0\n",
    "\n",
    "for session in model_args['dataset']['sessions']:\n",
    "    session_dir = os.path.join(CONFIG['data_dir'], session)\n",
    "    if os.path.exists(session_dir):\n",
    "        files = [f for f in os.listdir(session_dir) if f.endswith('.hdf5')]\n",
    "        eval_file = os.path.join(session_dir, f'data_{CONFIG[\"eval_type\"]}.hdf5')\n",
    "        \n",
    "        if os.path.exists(eval_file):\n",
    "            data = load_h5py_file(eval_file, b2txt_csv_df)\n",
    "            test_data[session] = data\n",
    "            total_test_trials += len(test_data[session][\"neural_features\"])\n",
    "            print(f'✓ Loaded {len(test_data[session][\"neural_features\"])} {CONFIG[\"eval_type\"]} trials for session {session}.')\n",
    "\n",
    "print(f'✓ Total number of {CONFIG[\"eval_type\"]} trials: {total_test_trials}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Inference (Phoneme Predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Run inference to get phoneme logits\n",
    "with tqdm(total=total_test_trials, desc='Predicting phoneme sequences', unit='trial') as pbar:\n",
    "    for session, data in test_data.items():\n",
    "        data['logits'] = []\n",
    "        input_layer = model_args['dataset']['sessions'].index(session)\n",
    "        \n",
    "        for trial in range(len(data['neural_features'])):\n",
    "            # Get neural input\n",
    "            neural_input = data['neural_features'][trial]\n",
    "            neural_input = np.expand_dims(neural_input, axis=0)\n",
    "            neural_input = torch.tensor(neural_input, device=device, dtype=torch.bfloat16)\n",
    "            \n",
    "            # Run decoding\n",
    "            logits = runSingleDecodingStep(neural_input, input_layer, model, model_args, device)\n",
    "            data['logits'].append(logits)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "print(\"✓ Inference completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Convert Logits to Phonemes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to phoneme sequences\n",
    "for session, data in test_data.items():\n",
    "    data['pred_seq'] = []\n",
    "    for trial in range(len(data['logits'])):\n",
    "        logits = data['logits'][trial][0]\n",
    "        pred_seq = np.argmax(logits, axis=-1)\n",
    "        # Remove blanks (0)\n",
    "        pred_seq = [int(p) for p in pred_seq if p != 0]\n",
    "        # Remove consecutive duplicates\n",
    "        pred_seq = [pred_seq[i] for i in range(len(pred_seq)) if i == 0 or pred_seq[i] != pred_seq[i-1]]\n",
    "        # Convert to phonemes\n",
    "        pred_seq = [LOGIT_TO_PHONEME[p] for p in pred_seq]\n",
    "        data['pred_seq'].append(pred_seq)\n",
    "\n",
    "print(\"✓ Phoneme conversion completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Predictions\n",
    "\n",
    "**Note:** The full language model pipeline requires Redis and additional setup. \n",
    "For Kaggle, you may need to use a simplified approach or implement phoneme-to-text conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions (simple version without language model)\n",
    "# This is a placeholder - you may need to implement phoneme-to-text conversion\n",
    "\n",
    "predictions = []\n",
    "ids = []\n",
    "\n",
    "for session in test_data.keys():\n",
    "    for trial in range(len(test_data[session]['pred_seq'])):\n",
    "        # Simple placeholder: join phonemes with spaces\n",
    "        # In practice, you'd want proper phoneme-to-text conversion\n",
    "        phoneme_seq = ' '.join(test_data[session]['pred_seq'][trial])\n",
    "        \n",
    "        # For now, use phoneme sequence as placeholder\n",
    "        # You should replace this with actual text prediction\n",
    "        predictions.append(phoneme_seq)\n",
    "        ids.append(len(ids))\n",
    "\n",
    "print(f\"✓ Generated {len(predictions)} predictions\")\n",
    "print(f\"\\nSample predictions:\")\n",
    "for i in range(min(5, len(predictions))):\n",
    "    print(f\"  {i}: {predictions[i][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Create submission DataFrame\n",
    "df_out = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'text': predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "output_file = CONFIG['output_file']\n",
    "df_out.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"✓ Predictions saved to {output_file}\")\n",
    "print(f\"\\nFirst few predictions:\")\n",
    "print(df_out.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluation (Validation Set Only)\n",
    "\n",
    "If evaluating on validation set, calculate Word Error Rate (WER).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['eval_type'] == 'val':\n",
    "    import editdistance\n",
    "    \n",
    "    total_true_length = 0\n",
    "    total_edit_distance = 0\n",
    "    \n",
    "    trial_idx = 0\n",
    "    for session in test_data.keys():\n",
    "        for trial in range(len(test_data[session]['sentence_label'])):\n",
    "            if trial_idx < len(predictions):\n",
    "                true_sentence = remove_punctuation(test_data[session]['sentence_label'][trial]).strip()\n",
    "                pred_sentence = remove_punctuation(predictions[trial_idx]).strip()\n",
    "                \n",
    "                ed = editdistance.eval(true_sentence.split(), pred_sentence.split())\n",
    "                total_true_length += len(true_sentence.split())\n",
    "                total_edit_distance += ed\n",
    "            trial_idx += 1\n",
    "    \n",
    "    if total_true_length > 0:\n",
    "        wer = 100 * total_edit_distance / total_true_length\n",
    "        print(f'Total true sentence length: {total_true_length}')\n",
    "        print(f'Total edit distance: {total_edit_distance}')\n",
    "        print(f'Aggregate Word Error Rate (WER): {wer:.2f}%')\n",
    "    else:\n",
    "        print(\"⚠ No validation data available for WER calculation\")\n",
    "else:\n",
    "    print(\"ℹ Test set evaluation - ground truth not available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "1. **Data Paths**: Update the paths in the CONFIG section (Cell 5) to match your Kaggle dataset structure.\n",
    "2. **Language Model**: The full language model pipeline requires Redis and additional setup. For Kaggle, you may need to use a simplified approach or implement phoneme-to-text conversion.\n",
    "3. **Phoneme-to-Text**: The current implementation uses phoneme sequences as placeholders. You'll need to implement proper phoneme-to-text conversion or use the language model.\n",
    "4. **File Structure**: Make sure your Kaggle dataset includes:\n",
    "   - Model checkpoint files (`checkpoint/args.yaml`, `checkpoint/best_checkpoint`)\n",
    "   - HDF5 data files (`hdf5_data_final/`)\n",
    "   - CSV metadata file (`t15_copyTaskData_description.csv`)\n",
    "   - Project source files (optional, if not defining inline)\n",
    "\n",
    "5. **Kaggle Dataset Setup**: \n",
    "   - Upload your model and data as Kaggle datasets\n",
    "   - Add them to your notebook via \"Add Data\" button\n",
    "   - Update the paths in CONFIG to point to `/kaggle/input/your-dataset-name/...`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
